{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d6bf20-aa3b-4b3d-86a2-e376bb5c9fc9",
   "metadata": {},
   "source": [
    "# Install or upgrade libraries\n",
    "\n",
    "It might be that you are running with the latest libraries and that they all work together fine. \n",
    "\n",
    "Running the following cell takes a minute or so but ensures that you have a consistent set of python tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c23a92-d96e-4717-9a70-ed5413167c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hist[plot]\n",
    "\n",
    "!pip install --upgrade pandas\n",
    "\n",
    "!pip install --upgrade matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1078c-4988-49af-a1e9-677847a448bd",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all the libraries we will need and check their versions, in case you run into issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397043f8-37f4-4021-8f39-c1a3eceb7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# The classics\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib # To get the version\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import hist\n",
    "from hist import Hist\n",
    "\n",
    "# To read file names using the OS (operating system)\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1e4e2-1ce7-4994-9d15-249ab8cab587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Versions --------\\n\")\n",
    "print(f\"{np.__version__ = }\\n\")\n",
    "print(f\"{matplotlib.__version__ = }\\n\")\n",
    "print(f\"{hist.__version__ = }\\n\")\n",
    "print(f\"{pd.__version__ = }\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71314ae-36d0-4197-951e-62a6cb86dccc",
   "metadata": {},
   "source": [
    "### Hist\n",
    "\n",
    "If you looked closely above, you'll see that we have a new library, \n",
    "[Hist](https://hist.readthedocs.io/en/latest/). It was developed in large part by\n",
    "members of the HEP communit and will help us with some plotting we'll be doing later.\n",
    "\n",
    "You don't *need* to use Hist and could instead use packages like [matplotlib](https://matplotlib.org/) on its own (as we will) or [seaborn](https://seaborn.pydata.org/), but Hist can make some things easier as we will see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49565c3-ffbc-419c-bfc6-915e508f5064",
   "metadata": {},
   "source": [
    "# Data-driven background estimation\n",
    "\n",
    "Even though we won't be using it with our $Z'$ search, we'll be startng with an example of *data-driven* approaches to background estimation. \n",
    "\n",
    "This will be a simple *side-band* estimate. In this example, there is a peak, sitting on top of some background. You can use more sophisticated approaches to estimating the background, but we will just look at how many events\n",
    "are in the region *not* under the peak and then use that to estimate the number of background events *under* the peak. \n",
    "\n",
    "Let's start by reading in some processed data from CMS open data. These are 100,000 events where there\n",
    "are two muons in the final state. We've gone ahead and calculated the combined 4-momenta for the two muons. \n",
    "\n",
    "We will read it in with [pandas](https://pandas.pydata.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a2556-6b46-4b8c-9cf4-8186036af79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/mattbellis/datasets_for_education/master/physics/dimuon_summary_data.csv', names=['e','px','py','pz','charge'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d6acd-469d-4df3-98d2-5ecbd9aadb08",
   "metadata": {},
   "source": [
    "The columns are the energy and three-momenta of the combined dimuon system, as well as the charge combinations: pp is when both muons are positive, mm is when they both have negative (minus) charge, and pm is when one is positive and one is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e2054-e989-4d6a-9bba-285b423a892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53599af1-3881-49ee-804d-8c005b98b4ee",
   "metadata": {},
   "source": [
    "We can calculate the mass with some simple, relativistic kinematics. \n",
    "\n",
    "$$E^2 = (pc)^2 + (mc^2)^2$$\n",
    "\n",
    "$$(mc^2)^2 = E^2 - (pc)^2$$\n",
    "\n",
    "$$(mc^2) = \\sqrt{E^2 - (pc)^2}$$\n",
    "\n",
    "And because our units are such that we can set $c=1$...\n",
    "\n",
    "$$m = \\sqrt{E^2 - p^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538f024-6230-4954-8a9e-ac1f087d9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the values\n",
    "e = data['e'].values\n",
    "px = data['px'].values\n",
    "py = data['py'].values\n",
    "pz = data['pz'].values\n",
    "\n",
    "# Calculate the mass\n",
    "mass = np.sqrt(e**2 - (px**2 + py**2 + pz**2))\n",
    "\n",
    "mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d70605-a51e-45b0-aee1-0d52c6a9cfed",
   "metadata": {},
   "source": [
    "We'll plot the data around the mass of the $J/\\psi$ meson. \n",
    "\n",
    "https://en.wikipedia.org/wiki/J/psi_meson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f38141-e490-4463-b65a-87c74eb6a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(mass,range=(2,4),bins=100)\n",
    "plt.xlabel(r'Mass of $\\mu\\mu$ pairs [GeV/c$^2$]',fontsize=14);\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimuon_masses_00.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682a88b-15b0-4b52-866f-d2246a9d2ed7",
   "metadata": {},
   "source": [
    "Let's pick out the events in the region where the peak is.\n",
    "\n",
    "This will select both signal events that populate the peak and the background events \"under\" the peak, since there is no way to distinguish on an event-by-event basis which is signal and which is background. \n",
    "\n",
    "In the examples below, I've purposely not been very careful about identifying what is or is not the peak. You can try to tweak my values to see if you can get a better estimate. \n",
    "\n",
    "We'll be making use of boolean masks to pull out subsets of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1436d8d-5bb4-47dd-a1b4-5f45facd58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the high/low values that pick out the peak\n",
    "peak_lo = 3.0\n",
    "peak_hi = 3.2\n",
    "\n",
    "# Make a boolean mask\n",
    "mass_peak_cut = (mass>peak_lo) & (mass<peak_hi)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(mass,range=(2,4),bins=100)\n",
    "plt.hist(mass[mass_peak_cut],range=(2,4),bins=100, color='r')\n",
    "plt.xlabel(r'Mass of $\\mu\\mu$ pairs [GeV/c$^2$]',fontsize=14);\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimuon_masses_01.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bf96c-c4ab-4d27-bb1f-3472ce12dc18",
   "metadata": {},
   "source": [
    "Now let's define the sidebands on either side of the peak. \n",
    "\n",
    "Note that we are taking care to make sure they are the same \"width\" as the peak region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda4dfe-2241-42b9-8aeb-9f609fe1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak boundaries\n",
    "peak_lo = 3.0\n",
    "peak_hi = 3.2\n",
    "\n",
    "# Sideband boundaries\n",
    "sb_left_lo = 2.8\n",
    "sb_left_hi = 3.0\n",
    "\n",
    "sb_right_lo = 3.2\n",
    "sb_right_hi = 3.4\n",
    "\n",
    "# Boolean bitmasks\n",
    "mass_peak_cut = (mass>peak_lo) & (mass<peak_hi)\n",
    "\n",
    "mass_sb_left_cut = (mass>sb_left_lo) & (mass<sb_left_hi)\n",
    "mass_sb_right_cut = (mass>sb_right_lo) & (mass<sb_right_hi)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(mass,range=(2,4),bins=100)\n",
    "plt.hist(mass[mass_peak_cut],range=(2,4),bins=100, color='r', label='Peak')\n",
    "plt.hist(mass[mass_sb_left_cut],range=(2,4),bins=100, color='y', label='Sidebands')\n",
    "plt.hist(mass[mass_sb_right_cut],range=(2,4),bins=100, color='y')\n",
    "\n",
    "plt.xlabel(r'Mass of $\\mu\\mu$ pairs [GeV/c$^2$]',fontsize=14);\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimuon_masses_02.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f9694-8bc7-47f7-b176-58e24d7bd6b6",
   "metadata": {},
   "source": [
    "We'll now count the entries in the sidebands ($N_{leftSB}$ and $N_{rightSB}$) and take the \n",
    "average ($N_{ave SB}$) as an estimate of the number of \n",
    "background events under the peak. \n",
    "\n",
    "We can then subtract that from the number of entries in the peak region ($N_{PR}$) to get an estimate of the\n",
    "number of signal events ($N_{sig}$). \n",
    "\n",
    "$$N_{aveSB} = \\frac{N_{leftSB} + N_{rightSB}}{2}$$\n",
    "\n",
    "$$N_{sig} = N_{PR} - N_{ave SB}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b412af-8ffd-4b3f-bc85-1415b8b7492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundaries\n",
    "peak_lo = 3.0\n",
    "peak_hi = 3.2\n",
    "\n",
    "sb_left_lo = 2.8\n",
    "sb_left_hi = 3.0\n",
    "\n",
    "sb_right_lo = 3.2\n",
    "sb_right_hi = 3.4\n",
    "\n",
    "# Cuts\n",
    "mass_peak_cut = (mass>peak_lo) & (mass<peak_hi)\n",
    "\n",
    "mass_sb_left_cut = (mass>sb_left_lo) & (mass<sb_left_hi)\n",
    "mass_sb_right_cut = (mass>sb_right_lo) & (mass<sb_right_hi)\n",
    "\n",
    "# Count the events\n",
    "N_peak = len(mass[mass_peak_cut])\n",
    "N_sb_left = len(mass[mass_sb_left_cut])\n",
    "N_sb_right = len(mass[mass_sb_right_cut])\n",
    "\n",
    "print(f\"N in peak region:    {N_peak}\")\n",
    "print(f\"N in left sideband:  {N_sb_left}\")\n",
    "print(f\"N in right sideband: {N_sb_right}\")\n",
    "print()\n",
    "\n",
    "# Calculate\n",
    "N_sb_ave = (N_sb_left + N_sb_right)/2\n",
    "print(f\"N sideband ave: {N_sb_ave}\")\n",
    "print()\n",
    "\n",
    "N_sig = N_peak - N_sb_ave\n",
    "print(f\"N signal:     {N_sig}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f520506-3adc-49a5-933d-9d284acfa4d7",
   "metadata": {},
   "source": [
    "How do you think we did? Can you do better?\n",
    "\n",
    "There are more sophisticated approaches to a data-driven background estimation, such as the \n",
    "[ABCD method](https://cms-opendata-guide.web.cern.ch/analysis/backgrounds/#abcd-method), \n",
    "which effectively extends the sidebands to a 2-dimensional plane of some kinematic variables. \n",
    "\n",
    "It's up to you to determine which approach is best for your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b007e-dbcd-4d5e-b3e5-e0833c8bc3e0",
   "metadata": {},
   "source": [
    "# Simulation-based background estimation\n",
    "\n",
    "Another approach is to use a \n",
    "[simulation based](https://cms-opendata-guide.web.cern.ch/analysis/backgrounds/#simulation) estimate\n",
    "of your backgrounds and that is what we will use in our $Z'$ search. \n",
    "\n",
    "Basically this is making use of our simulation datasets that model standard model processes, the backgrounds\n",
    "for our search for new physics. We add up all of these contributions and see if there is an excess that \n",
    "could be explained by our signal hypothesis. \n",
    "\n",
    "However, we need to take care that we are using our background Monte Carlo in a way that we can do an actual\n",
    "comparison with the collision data. \n",
    "\n",
    "## Download the processed files\n",
    "\n",
    "We're going to build upon the data processing we did in the [previous lesson on event selection](https://cms-opendata-workshop.github.io/workshop2024-lesson-event-selection/instructor/index.html).\n",
    "\n",
    "We've processed all the files for you, using the exact same code as in that lesson. Let's get those files now. \n",
    "\n",
    "They are in a [tar file](https://en.wikipedia.org/wiki/Tar_(computing)) and when we untar it, it will \n",
    "create a subdirectory called `processed_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96c0c-d08e-473a-abe3-9a6bb531c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tar file\n",
    "!wget https://github.com/cms-opendata-workshop/workshop2024-lesson-event-selection/raw/main/instructors/processed_files.tgz\n",
    "\n",
    "# Untar it\n",
    "# Verbose output\n",
    "#!tar -zxvf processed_files.tgz\n",
    "\n",
    "# Silent output\n",
    "!tar -zxf processed_files.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd34be-a53a-4eff-997a-0bdd3a431c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can list our directory contents now\n",
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de8348-c86f-4331-9300-d34a7a695eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look in the directory\n",
    "!ls processed_files | tail -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adc3f7-a321-44e8-b8c4-2244a8c97886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many files are in the `processed_files` directory\n",
    "!ls processed_files | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b67818-d466-4aed-9fcf-eb6323bd7d18",
   "metadata": {},
   "source": [
    "We processed 555 files out of all our datasets. As a reminder...\n",
    "\n",
    "`collision` refers to the data and the other names refer to the signal MC and background MC samples. \n",
    "\n",
    "* Signal MC datasets\n",
    "  * `signal_M2000`\n",
    "* Background MC datasets\n",
    "  * `ttsemilep`\n",
    "  * `tthadronic`\n",
    "  * `ttleptonic`\n",
    "  * `Wjets`\n",
    "\n",
    "What is in these files? We can open them and read them in as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a2a42-41d5-434e-ac3b-033a86a2fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one of the collision files\n",
    "df_col = pd.read_csv('processed_files/OUTPUT_collision_0107961B-4308-F845-8F96-E14622BBA484.csv')\n",
    "\n",
    "# Display the first 5 rows\n",
    "df_col.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cd50e-65ce-413a-9939-7ebba66e4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one of the simulation files\n",
    "df_sim = pd.read_csv('processed_files/OUTPUT_tthadronic_009086DB-1E42-7545-9A35-1433EC89D04B.csv')\n",
    "\n",
    "# Display the first 5 rows\n",
    "df_sim.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308d352-38fa-4460-8e98-157653b17948",
   "metadata": {},
   "source": [
    "555 files is a lot of files to keep track of! \n",
    "\n",
    "We're going to merge these together, so we wind up with 1 files per dataset. \n",
    "\n",
    "We have to keep track of the number of events we started with in a slightly more sophisticated fashion, \n",
    "but otherwise all we're doing is concatenating these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60736f-2ae6-4760-ba37-83b8eb5e750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['collision', 'signal_M2000', 'ttsemilep', 'tthadronic', 'ttleptonic', 'Wjets']\n",
    "\n",
    "collision_datasets = datasets[0]\n",
    "signal_datasets = datasets[1]\n",
    "background_datasets = datasets[2:]\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    N_gen = 0\n",
    "    nevents = 0\n",
    "    gw_pos = 0\n",
    "    gw_neg = 0\n",
    "    \n",
    "    IS_COLLISION = False\n",
    "    \n",
    "    path = r'./' # use your path\n",
    "    all_files = glob.glob(os.path.join(path , f\"processed_files/OUTPUT_{dataset}*.csv\"))\n",
    "    \n",
    "    print(f\"Processing {len(all_files)} files in {dataset} \")\n",
    "\n",
    "    list_of_dataframes = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        \n",
    "        if filename.find('collision') >= 0:\n",
    "            IS_COLLISION = True\n",
    "            \n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "\n",
    "        list_of_dataframes.append(df)\n",
    "\n",
    "        if len(df) > 0:\n",
    "            N_gen += df['N_gen'][0]\n",
    "            nevents += df['nevents'][0]\n",
    "            gw_pos += df['gw_pos'][0]\n",
    "            gw_neg += df['gw_neg'][0]\n",
    "\n",
    "    if IS_COLLISION:\n",
    "        N_gen = -999\n",
    "        gw_pos = -999\n",
    "        gw_neg = -999\n",
    "\n",
    "    df = pd.concat(list_of_dataframes, axis=0, ignore_index=True)\n",
    "    df['N_gen'] = N_gen\n",
    "    df['nevents'] = nevents\n",
    "    df['gw_pos'] = gw_pos\n",
    "    df['gw_neg'] = gw_neg\n",
    "    \n",
    "    df.to_csv(f'SUMMED_{dataset}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcd59f-f1d8-45ac-87a5-5fc8e9de0e84",
   "metadata": {},
   "source": [
    "We now have a set of files called `SUMMED_ttsemilep.csv` for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc3b61-77f4-4cc9-9b2e-a1775359929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55056d-de35-456a-8e88-30e5bd35680d",
   "metadata": {},
   "source": [
    "We can see how many events remain in each dataset, just by getting the number of lines in each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288fe0a-0eed-45cf-87af-69afd9526216",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l SUMMED*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4035a-a91c-4cef-ad3b-e64d3c81d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head SUMMED_collision.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123034bb-630b-46f2-9f4f-af61f04a2fdf",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "We're going to be talking a lot about weights so lets explain what we mean and how these go into our histograms. \n",
    "\n",
    "A weight means that any individual event might count for more or less in our analysis. This could be because\n",
    "of how they were generated or how they were processed. \n",
    "\n",
    "When we fill a histogram we can choose to weight each event, or some set of events by some amount. \n",
    "\n",
    "Here's an example with 2 events that go into a histogram with equal weight=1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d2172-4578-4d30-a256-89df62c773ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data\n",
    "x = [5, 10]\n",
    "\n",
    "# The plot\n",
    "plt.hist(x,bins=10, range=(0,10))\n",
    "plt.ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f9991-6d2a-44dc-8003-c6008b51b45e",
   "metadata": {},
   "source": [
    "Now we weight it such that one entry has a weight of 0.5 and the other has a weight of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0b45d-b916-4515-9eba-075ccc01177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data\n",
    "x = [5, 10]\n",
    "\n",
    "# Our weights\n",
    "weights = [0.5, 2]\n",
    "\n",
    "# The plot\n",
    "plt.hist(x, weights=weights, bins=10, range=(0,10))\n",
    "plt.ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f30072-04e4-4022-a235-77a3b9f13cc0",
   "metadata": {},
   "source": [
    "We can also use this to save on computing power. For example, suppose\n",
    "I am studying some particle that has a mass of 3.1 and a width of 0.1, if we model it\n",
    "with a normal distribution (Gaussian). We can generate 100k events quite easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84355443-0a93-44fb-ae65-42d13b8c64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "signal = np.random.normal(3.1, 0.1,100000)\n",
    "\n",
    "# Plot the data\n",
    "plt.hist(signal,bins=100, range=(2.6,3.6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967c56a-2076-4d2f-b86e-fb158cc700ac",
   "metadata": {},
   "source": [
    "But suppose this is our background and we are interested in some other process that might\n",
    "manifets around 3.4 or 3.5. Well, there are almost no entries out there and we don't want to generate a billion events, just to get good statistics out there. \n",
    "\n",
    "Instead, we can generate a uniform distribution of events and then calculate the *weight* for each event, \n",
    "if we assume that it comes from a normal distribution. The weight is just the normalized probability density\n",
    "for that event coming from a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367226a-398c-4e0f-8c00-bf65fec2a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate the normalized probability density\n",
    "def norm(mean, sigma, x):\n",
    "    a = 1/(sigma*np.sqrt(2*np.pi))\n",
    "    pdf = a*np.exp((-(x-mean)**2)/(2*(sigma**2)))\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516018a-baf8-431a-96f8-f7b2568e725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our flat distribution of data\n",
    "signal_flat = 2.6 + np.random.random(100000)\n",
    "\n",
    "# The plot\n",
    "plt.hist(signal_flat,bins=100, range=(2.6,3.6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae4941-fb15-4f96-9b3b-4ad6dbd81d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to calculate the probability density, which we store as the weight\n",
    "# for each event\n",
    "\n",
    "weights = norm(3.1,0.1, signal_flat)\n",
    "weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273ac47-d4da-4d1c-97a2-0ff63c09e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we plot the weights, they don't seem to tell us anything\n",
    "plt.hist(weights,bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf3500-560b-46a3-ab83-6bf992dbaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we histogram the data and use the weights, we get our Gaussian back\n",
    "plt.hist(signal_flat, weights=weights,bins=100, range=(2.6,3.6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf59cc3-cf50-4dee-852d-926d9c07b11f",
   "metadata": {},
   "source": [
    "### Back to the data\n",
    "\n",
    "Let's read in all our `.csv` files as dataframes and store them in a dictionary. \n",
    "\n",
    "We will also pull out the `N_gen` variable from the file and store that in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e498c-92c0-4bd2-8d2f-bfa25d78cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'SUMMED_{dataset}.csv')\n",
    "    \n",
    "    N_gen = df['N_gen'][0]\n",
    "    \n",
    "    data[dataset] = {'df':df, 'N_gen': N_gen}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53109fa5-2a1b-4384-99ba-d00319ac4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d188d-b37d-4357-87bb-12159ed980c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access subsets of the dictionary to get individual datasets\n",
    "data['ttsemilep']['df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d88988-98a0-4ba8-9ee5-49b4065ff13c",
   "metadata": {},
   "source": [
    "We can also compare our total number of events (either `N_gen` or `nevents`) with the\n",
    "offical number on the Open Data portal. \n",
    "\n",
    "https://opendata.cern.ch/record/67993\n",
    "\n",
    "Any small differences are usually attributable to the fact that we did not process every file perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee6617-efee-43c7-bd0d-072a2cf529b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttsemilep']['N_gen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bb182-2acb-4c46-8fcd-955e55efa235",
   "metadata": {},
   "source": [
    "We also need to fix something! \n",
    "\n",
    "A mistake was made when we combined all the `Wjets` files. Because some NanoAOD files had no events survive, \n",
    "those `.csv` files were empty and we never recorded the original number of generated events for those input files. \n",
    "\n",
    "To fix this, we will just set the value using the information from the Open Data Portal. \n",
    "\n",
    "https://opendata.cern.ch/record/69747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7b2a0-50b1-4dea-8fa5-4f0341e148d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mea culpa\n",
    "\n",
    "data['Wjets']['N_gen'] = 80958227\n",
    "data['Wjets']['df']['N_gen'] = 80958227\n",
    "\n",
    "data['Wjets']['N_gen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c490aa-49e6-455e-a0fa-d1d885f80f53",
   "metadata": {},
   "source": [
    "Now we can make a quick plot of all the different datasets!\n",
    "\n",
    "Do any of them look different? Similar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe49d9-4920-4c0b-9d7d-c8bc496b157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "for i,dataset in enumerate(datasets):\n",
    "    vals = data[dataset]['df']['mtt'].values\n",
    "    \n",
    "    plt.subplot(2,3,i+1)\n",
    "    \n",
    "    plt.hist(vals, bins=40, range=(0,4000), label=dataset)\n",
    "    plt.xlabel(f'$M_{{t\\overline{{t}}}}$ GeV/c$^2$', fontsize=14)\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812fe58-2711-46f6-af3a-61d7ab643286",
   "metadata": {},
   "source": [
    "### Using `Hist`\n",
    "\n",
    "We eventually want to compare the stacked background histograms with the collision data and you \n",
    "can do that with vanilla matplotlib or ROOT. \n",
    "\n",
    "However, we're going to use the [Hist](https://hist.readthedocs.io/en/latest/) library as it has some nice features. \n",
    "\n",
    "First, we make an empty histogram that allows us to save the data and put it into categories that depend on the dataset. \n",
    "\n",
    "It will have 40 bins and range from 0 to 4000 for the `mtt` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098dfa48-f505-47dc-a61e-94aeea27b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hist.new.Reg(40,0,4000, name='mtt', label=f'$M_{{t\\overline{{t}}}}$ GeV/c$^2$')\\\n",
    "                .StrCat([], name=\"dataset\", label=\"dataset\", growth=True)\\\n",
    "                .Weight()\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4d738-3e5f-4107-ad46-0102bcd28320",
   "metadata": {},
   "source": [
    "Now we can fill it quite easily! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b724e-6abb-4e51-b822-82341f040793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the datasets\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(dataset)\n",
    "\n",
    "    # Extract the mtt values from our dictionary\n",
    "    vals = data[dataset]['df']['mtt'].values\n",
    "\n",
    "    # Fill the histogram and label by dataset\n",
    "    h.fill(mtt=vals, dataset=dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890d308-bc90-4f32-9916-376ec33693b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hypersimple representation of what is in the histogram\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d4d3d-87c1-46ab-bdc0-485b5d303416",
   "metadata": {},
   "source": [
    "Now that it is all in the histogram object, we can quickly stack the background datasets\n",
    "and compare to our data. \n",
    "\n",
    "As a reminder we defined subsets of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fccd83-3931-4866-a5ca-ec036c5347a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collision datasets\")\n",
    "print(collision_datasets)\n",
    "print()\n",
    "\n",
    "print(\"Signal datasets\")\n",
    "print(signal_datasets)\n",
    "print()\n",
    "\n",
    "print(\"Background datasets\")\n",
    "print(background_datasets)\n",
    "print()\n",
    "\n",
    "print(\"All datasets\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804cff7-29f1-4044-bf22-23f30cc4de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Stack the background datasets\n",
    "h[:,background_datasets].stack('dataset')[:].project('mtt').plot(stack=True, histtype=\"fill\")\n",
    "\n",
    "# Overlay the collision datasets\n",
    "h[:,collision_datasets].project('mtt').plot(histtype=\"errorbar\", color='k', markersize=15, label='Collision')\n",
    "\n",
    "# Increase the size of the x-axis label\n",
    "plt.xlabel(xlabel=plt.gca().get_xlabel(), fontsize=18);\n",
    "\n",
    "# Draw a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mtt_00.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa539837-110e-4825-8730-0f981b9ae8a2",
   "metadata": {},
   "source": [
    "Oh no!!! This looks *terrible*! The MC doesn't agree with the data at all!\n",
    "\n",
    "Not surprising. We haven't weighted our MC samples appropriately. We tend to generate\n",
    "more MC than you would expect to find in the data. So we need to take into account\n",
    "\n",
    "* What is the integrated luminosity ($\\mathcal{L}$) for the collision data we're working with?\n",
    "    * 16400 inverse picobarns\n",
    "* What is the cross section ($\\sigma$) for these samples?\n",
    "* How many events were generated for each sample?\n",
    "\n",
    "We can then calculate a weight for each background sample. \n",
    "\n",
    "$$\\textrm{weight} = \\frac{\\mathcal{L} \\times \\sigma}{N_{gen}}$$\n",
    "\n",
    "For the signal, we will assume a cross section of 1 picobarn and then let the inference procedure tell us\n",
    "what is the value that best described the data. \n",
    "\n",
    "For the `ttbar` backgrounds, we start with the production cross section for top/antitop production \n",
    "(831.76) and then scale by the probability of the W's decaying hadronically, leptonically, or one and the other. \n",
    "\n",
    "For the collision data, we just use a weight of 1.\n",
    "\n",
    "We'll calculate this for each dataset and store it in our `data` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27ca4f-336a-4a52-b7c3-7a402aaa40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_luminosity = 16400 # inverse picobarns\n",
    "\n",
    "ttbar_production_xsec = 831.76 # picobarns\n",
    "\n",
    "data['Wjets']['xsec'] = 61526.7\n",
    "data['ttsemilep']['xsec'] = ttbar_production_xsec*0.438\n",
    "data['tthadronic']['xsec'] = ttbar_production_xsec*0.457\n",
    "data['ttleptonic']['xsec'] = ttbar_production_xsec*0.105\n",
    "data['signal_M2000']['xsec'] = 1 # Assume 1 pb\n",
    "\n",
    "\n",
    "for dataset in [signal_datasets] + background_datasets:\n",
    "    \n",
    "    N_gen = data[dataset]['N_gen']\n",
    "    \n",
    "    xsec = data[dataset]['xsec']\n",
    "    \n",
    "    weight = integrated_luminosity * xsec / N_gen\n",
    "    \n",
    "    data[dataset]['weight'] = weight\n",
    "    \n",
    "data['collision']['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face79a5-d84c-4885-a3e8-f640bc469668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print them out \n",
    "for dataset in datasets:\n",
    "    w = data[dataset]['weight']\n",
    "    \n",
    "    print(f'{dataset:20s}  {w:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488200ce-3d4a-44c7-81aa-c16c11f0fb4b",
   "metadata": {},
   "source": [
    "Let's redefine the Hist histograms and refill them, but this time we will use the weights we calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e24a1-abd5-4207-acec-7c7023492ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hist.new.Reg(40,0,4000, name='mtt', label=f'$M_{{t\\overline{{t}}}}$ GeV/c$^2$')\\\n",
    "                .StrCat([], name=\"dataset\", label=\"dataset\", growth=True)\\\n",
    "                .Weight()\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(dataset)\n",
    "\n",
    "    vals = data[dataset]['df']['mtt'].values\n",
    "    weight = data[dataset]['weight']\n",
    "\n",
    "\n",
    "    h.fill(mtt=vals, dataset=dataset, weight=weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaf2e4-fb05-45ef-84f3-b20f80e897bf",
   "metadata": {},
   "source": [
    "Now we can make the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bdcd8-b00f-4237-8e9b-1e0061400e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Stack the background datasets\n",
    "h[:,background_datasets].stack('dataset')[:].project('mtt').plot(stack=True, histtype=\"fill\")\n",
    "\n",
    "# Overlay the collision datasets\n",
    "h[:,collision_datasets].project('mtt').plot(histtype=\"errorbar\", color='k', markersize=15, label='Collision')\n",
    "\n",
    "# Increase the size of the x-axis label\n",
    "plt.xlabel(xlabel=plt.gca().get_xlabel(), fontsize=18);\n",
    "\n",
    "# Draw a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mtt_01.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d96c2-42c3-4d1e-8846-7e6e82086c03",
   "metadata": {},
   "source": [
    "That's pretty good agreement for our zeroth-order reproduction of the analysis!\n",
    "\n",
    "Let's overlay the signal to see where that would manifest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5f05d-a116-4238-b15c-27fb84f18fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Stack the background datasets\n",
    "h[:,background_datasets].stack('dataset')[:].project('mtt').plot(stack=True, histtype=\"fill\")\n",
    "\n",
    "# Overlay the collision datasets\n",
    "h[:,collision_datasets].project('mtt').plot(histtype=\"errorbar\", color='k', markersize=15, label='Collision')\n",
    "\n",
    "# Overlay the signal hypothesis\n",
    "h[:,signal_datasets].project('mtt').plot(color='y', lw=5, label=f\"$Z'$ (M=2000 GeV/c$^2$\")\n",
    "\n",
    "# Increase the size of the x-axis label\n",
    "plt.xlabel(xlabel=plt.gca().get_xlabel(), fontsize=18);\n",
    "\n",
    "# Draw a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mtt_02.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303dc05-5e15-4f59-ac43-67079c5a86f6",
   "metadata": {},
   "source": [
    "From this we can see a few things\n",
    "\n",
    "* Scaling the backgrounds appropriately does a pretty good job of modeling the collision data\n",
    "* The background are primarily coming from the semileptonic decays of top/antitop pairs\n",
    "* The signal peaks differently from the background\n",
    "* There is no *obvious* signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee4693-0606-4d6d-a51b-edde9808efd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724e499-c147-428f-9174-9876676b64f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
